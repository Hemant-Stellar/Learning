# -*- coding: utf-8 -*-
"""unit4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNZQyXD1_kFYy3w2gaskgJdHwQClsl4D

### **SENTIMENT ANALYSIS WITH **NEURAL** NETWORK **
"""

import numpy as np
from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

# Load the IMDb dataset
max_features = 10000  # Consider only the top 10,000 most frequently occurring words in the dataset
max_len = 200  # Cut or pad all reviews to be exactly 200 words long to maintain uniformity

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# Preprocess the data: pad sequences to the same length
x_train = pad_sequences(x_train, maxlen=max_len)  # Ensures all reviews are of equal length
x_test = pad_sequences(x_test, maxlen=max_len)    # Makes the model training and testing efficient

# Build the neural network model
model = Sequential()

# Embedding layer: Converts words into dense vectors of fixed size
# input_dim = max_features: Vocabulary size (top 10,000 words)
# output_dim = 128: Each word will be represented as a 128-dimensional vector
# input_englth = max_len: Input sequences are all of length 200
model.add(Embedding(input_dim=max_features,
          output_dim=128, input_length=max_len))

# Flatten layer: Converts the 2D output of Embedding into a 1D vector
# This is necessary for feeding into the Dense layer
model.add(Flatten())

# Dense layer: Fully connected layer with 32 neurons
# Relu activation function is commonly used in hidden layers
model.add(Dense(32, activation='relu'))

# Dense output layer: A single neuron with a sigmoid activation function
# Sigmoid function outputs probabilities between 0 and 1, suitable for binary classification
model.add(Dense(1, activation='sigmoid'))

# Compile the model
# Optimizer: 'adam' works well for many NLP problems
# Loss: 'binary_crossentropy' since it's a binary classification task
# Metrics: 'accuracy' to track the model's performance
model.compile(optimizer='adam',
    loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
# epochs = 5: Train for 5 iterations over the entire training set
# batch_size = 32: Use 32 samples per gradient update; a common choice for efficient training
# validation_split = 0.2: 20% of the training data will be used for validation to monitor performance
model.fit(x_train, y_train, epochs=5, batch_size=32,
          validation_split=0.2)
loss, accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

"""### **RNN for Language modelling**"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.optimizers import Adam

# Define vocabulary size, embedding dimension, and hidden state dimension
vocab_size = 10  # Total number of unique words in the vocabulary
embedding_dim = 5  # Size of the embedding vector for each word
hidden_dim = 8  # Number of units in the hidden layer of the RNN

# Create the model
model = Sequential()  # Initialize a sequential model

# Add an Embedding layer
# input_dim = vocab_size -> The size of the input vocabulary (10 unique words)
# output_dim = embedding_dim -> Dimension of the dense embedding vectors (5)
# input_length = 3 -> The length of input sequences (3 words per sequence in this example)
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=3))

# Add a SimpleRNN layer
# hidden_dim = 8 -> Number of RNN units, which determines the dimensionality of the output space
# return_sequences=True -> Ensures that the RNN layer outputs a sequence for each time step
model.add(SimpleRNN(hidden_dim, return_sequences=True))

# Add a Dense layer
# hidden_dim -> The input dimension from the RNN's hidden state
# vocab_size = 10 -> Output dimension to match the vocabulary size for prediction
# activation='softmax' -> Activation function to get probability distribution over vocabulary
model.add(Dense(vocab_size, activation='softmax'))

# Compile the model
# loss='sparse_categorical_crossentropy' -> Suitable for integer targets (word indices)
# optimizer=Adam() -> Adam optimizer for efficient training
# metrics=['accuracy'] -> Track accuracy during training
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

# Example input data (batch of sequences)
# 2 sequences with each sequence containing 3-word indices
input_data = np.array([[1, 2, 3], [4, 5, 6]])  # Example batch size = 2, sequence length = 3

# Example target data that corresponds to input sequences
# Each value represents the correct next word index for each time step
target_data = np.array([[2, 3, 1], [5, 6, 4]])  # Batch size = 2, sequence length = 3

# Train the model
# epochs=5 -> Number of times to iterate over the training data
# verbose=1 -> Display progress during training
history = model.fit(input_data, target_data, epochs=5, verbose=1)

# Retrieve accuracy values from training history
accuracy = history.history['accuracy']

# Print the accuracy for each epoch
# 'enumerate' is used to get epoch number starting from 1
for epoch, acc in enumerate(accuracy, 1):
    print(f"Accuracy after epoch {epoch}: {acc:.4f}")

"""### **RNN for sentiment analysis**"""

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Set parameters
vocab_size = 10000  # Number of words to consider (most frequent)
embedding_dim = 32  # Size of the embedding vectors
max_length = 100     # Maximum length of input sequences

# Load the IMDB dataset
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# Pad sequences to ensure uniform input size
x_train = pad_sequences(x_train, maxlen=max_length)
x_test = pad_sequences(x_test, maxlen=max_length)

# Create the RNN model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))  # Embedding layer
model.add(SimpleRNN(32))  # RNN layer with 32 units
model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)  # Use 20% of training data for validation

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_accuracy:.4f}')

"""### **GRU**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the IMDb dataset
# This loads the dataset and splits it into training and testing sets.
# The num_words parameter limits the dataset to the top 10,000 most frequent words.
(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)

# Pad the sequences to a maximum length of 256
# Padding ensures all sequences are the same length, which is necessary for batch processing in neural networks.
maxlen = 256
train_data = pad_sequences(train_data, maxlen=maxlen)  # Pad training data
test_data = pad_sequences(test_data, maxlen=maxlen)    # Pad testing data

# Print the shape of the training and testing data
print(f'Shape of training data: {train_data.shape}')  # Expected: (number_of_training_samples, 256)
print(f'Shape of test data: {test_data.shape}')        # Expected: (number_of_test_samples, 256)

# Define the model
model = Sequential()  # Initialize a sequential model

# Embedding Layer
# input_dim: Size of the vocabulary (number of unique words). Here, it's 10,000.
# output_dim: Dimension of the dense embedding. Here, each word is represented by a 128-dimensional vector.
# input_length: Length of input sequences (256), as padded earlier.
model.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen))

# GRU Layer
# Units: The number of memory units in the GRU layer. Here, we use 64 units to capture temporal dependencies.
model.add(GRU(64))

# Dense Layer
# Units: The output size. Here, it's 1 for binary classification (positive or negative sentiment).
# activation: Sigmoid function outputs a value between 0 and 1, interpreted as the probability of a positive review.
model.add(Dense(1, activation='sigmoid'))

# Compile the model
# optimizer: 'adam' is a popular optimizer that adapts the learning rate during training.
# loss: 'binary_crossentropy' is used for binary classification problems.
# metrics: We track accuracy as the performance metric during training.
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
# epochs: The number of times the model will go through the entire training dataset. Here, it's set to 5.
# batch_size: Number of samples processed before the model's weights are updated. Here, we use 64.
# validation_split: Fraction of the training data to be used as validation data (20% in this case).
model.fit(train_data, train_labels, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate the model
# This evaluates the model's performance on the test data.
loss, accuracy = model.evaluate(test_data, test_labels)
print(f'Test accuracy: {accuracy:.4f}')  # Print the accuracy of the model on the test set.

"""T

### **Named entity recognition**
"""

import spacy

# Load the pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

# Sample text
text = "Apple Inc. is planning to open a new office in Bangalore in January 2025. Lovely Professional University"

# Process the text with spaCy
doc = nlp(text)

# Extract named entities
for entity in doc.ents:
    print(f"Entity: {entity.text} | Label: {entity.label_}")

"""### ***LSTM***"""

import numpy as np
from tensorflow.keras.datasets import imdb  # IMDB dataset for binary sentiment analysis
from tensorflow.keras.models import Sequential  # Sequential model for building neural networks layer by layer
from tensorflow.keras.layers import Embedding, LSTM, Dense  # Layers for embedding, LSTM, and dense output
from tensorflow.keras.preprocessing.sequence import pad_sequences  # Function to pad sequences to a uniform length

# Load the IMDB dataset
vocab_size = 10000  # Number of unique words to keep based on frequency in the dataset
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)  # Loading training and test data with top 10,000 frequent words

# Set maximum review length and pad sequences
max_length = 100  # Each review will be padded/truncated to 100 words
x_train_padded = pad_sequences(x_train, maxlen=max_length, padding='post')  # Pads sequences at the end ('post') to ensure uniform length
x_test_padded = pad_sequences(x_test, maxlen=max_length, padding='post')    # Applies the same padding to test data

# Build the LSTM model
model = Sequential()  # Initializing a Sequential model to stack layers

# Embedding layer
model.add(Embedding(input_dim=vocab_size,  # Vocabulary size (number of unique words considered)
                    output_dim=64,        # Dimension of the dense embedding vectors
                    input_length=max_length))  # Maximum length of input sequences

# LSTM layer
model.add(LSTM(128))             # Number of LSTM units (memory cells) to capture long-term dependencies
               # Output only the final hidden state (not sequences for each input)

# Output layer
model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 unit and 'sigmoid' activation for binary classification (positive/negative)

# Compile the model
model.compile(optimizer='adam',  # 'adam' optimizer for adaptive learning rates during training
              loss='binary_crossentropy',  # 'binary_crossentropy' as the loss function for binary classification
              metrics=['accuracy'])  # Evaluate model performance using accuracy

# Train the model
model.fit(x_train_padded, y_train,  # Training data and labels
          epochs=3,               # Number of complete passes through the training data
          batch_size=64,           # Number of samples processed before the model updates weights
          validation_data=(x_test_padded, y_test))  # Validation data for checking model performance during training

# Evaluate the model on test data
loss, accuracy = model.evaluate(x_test_padded, y_test)  # Calculate loss and accuracy on the test dataset
print(f"Test Loss: {loss}")     # Display test loss
print(f"Test Accuracy: {accuracy}")  # Display test accuracy

"""### **siamese network**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv2D,MaxPooling2D, Flatten, Lambda
from tensorflow.keras import backend as K
# Input shape for the images (105x105 pixels, 1 channel for grayscale images)
input_shape = (105, 105, 1)
# Function to create the base network architecture for feature extraction
def create_base_network(input_shape):
    input = Input(shape=input_shape)
    x = Conv2D(64, (10, 10), activation='relu')(input)
    x = MaxPooling2D()(x)
    x = Conv2D(128, (7, 7), activation='relu')(x)
    x = MaxPooling2D()(x)
    x = Conv2D(128, (4, 4), activation='relu')(x)
    x = MaxPooling2D()(x)
    x = Flatten()(x)
    x = Dense(256, activation='relu')(x)
    return Model(input, x)
# Define two input layers for the two images we want to compare
input_a = Input(shape=input_shape)
input_b = Input(shape=input_shape)
# Instantiate the base network
base_network = create_base_network(input_shape)
# Process both inputs through the shared base network
processed_a = base_network(input_a)
processed_b = base_network(input_b)
# Lambda layer to compute the L1 distance (absolute difference) between the two feature vectors
distance = Lambda(lambda x: K.abs(x[0] - x[1]))([processed_a, processed_b])
# Output layer
output = Dense(1, activation='sigmoid')(distance)
# Define the final Siamese network model
siamese_network = Model(inputs=[input_a, input_b], outputs=output)
# Compile the model
siamese_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Display the model's architecture
siamese_network.summary()
# Generate dummy data for training (for illustration purposes)
num_samples_train = 1000
image_a_train = np.random.rand(num_samples_train, 105, 105, 1)
image_b_train = np.random.rand(num_samples_train, 105, 105, 1)
labels_train = np.random.randint(0, 2, num_samples_train)
# Train the model
history = siamese_network.fit([image_a_train, image_b_train],
                              labels_train, epochs=2, batch_size=64)
# Print training accuracy
print(f'Training Accuracy: {history.history["accuracy"][-1]:.4f}')
# Generate dummy data for testing
num_samples_test = 200
image_a_test = np.random.rand(num_samples_test, 105, 105, 1)
image_b_test = np.random.rand(num_samples_test, 105, 105, 1)
labels_test = np.random.randint(0, 2, num_samples_test)
# Create a model to output the distance
distance_model = Model(inputs=[input_a, input_b], outputs=distance)
# Get distances for the test set
distances = distance_model.predict([image_a_test, image_b_test])
# Evaluate the model on the test set
test_loss, test_accuracy = siamese_network.evaluate([image_a_test, image_b_test], labels_test)
# Print testing accuracy
print(f'Testing Accuracy: {test_accuracy:.4f}')
# Print distances for each test sample
print("Distances for each test sample:")
for i in range(10):  # Print distances for the first 10 samples
    print(f'Sample {i + 1}: Distance = {distances[i][0]:.4f}, Label = {labels_test[i]}')

import pandas as pd

# Create a sample dataset
data = {
    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David'],
    'Age': [25, 30, 25, 35, 30, 40],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'Houston']
}

df = pd.DataFrame(data)
print("Original DataFrame:")
print(df)

# Identify duplicates
duplicates = df.duplicated()
print("\nDuplicate Rows:")
print(df[duplicates])

# Count duplicates
num_duplicates = duplicates.sum()
print(f"\nNumber of duplicate rows: {num_duplicates}")

# Remove duplicates
df_unique = df.drop_duplicates()
print("\nDataFrame after removing duplicates:")
print(df_unique)